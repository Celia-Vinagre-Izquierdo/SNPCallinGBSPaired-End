Supplemental Code for:

Chromosomal inversion associated with dietary niche differences in common quails sharing wintering grounds

Celia Vinagre-Izquierdo, Ines Sanchez-Donoso, Jennifer A. Leonard, José Domingo Rodríguez-Teijeiro and Carles Vilà. 
  
Corresponding authors: Carles Vilà (carles.vila@ebd.csic.es)

Created by Celia Vinagre-Izquierdo (Feb 2025) 


#######################################################
##### 	    Demultiplex raw sequence data         #####
#######################################################

# Axe-demux for demultiplexing
axe-demux -m0 -z 3 -c -2 -v -t summary.txt \
         -b ./PstI-Plate1Key.txt \
         -f *_R1.fastq.gz -r *_R2.fastq.gz -F gbs -R gbs

#######################################################
##### 	            Remove adapters		  #####
#######################################################

# Script to process paired-end reads with trim_galore
#!/bin/bash

# Directory containing input FASTQ files
input_dir="/path/to/fastq/files/"

# Loop through all R1 files
for r1_file in ${input_dir}*_R1.fastq.gz; do
    # Extract the sample name from the R1 filename
    sample_name=$(basename "$r1_file" _R1.fastq.gz)

    # Construct the corresponding R2 filename
    r2_file="${input_dir}${sample_name}_R2.fastq.gz"

    # Check if the corresponding R2 file exists
    if [ -f "$r2_file" ]; then
        echo "Processing $sample_name..."
        trim_galore --paired -e 0.1 -q 20 --fastqc --length 30 "$r1_file" "$r2_file"
    else
        echo "Error: Corresponding R2 file not found for $r1_file"
    fi
done

# Run FastQC on trimmed files
fastqc *_trimmed*

chmod +x trimgalore_paired.sh


#######################################################
##### 	          Merge paired-end reads          #####
#######################################################

# Merge paired-end reads using bbmerge.sh
#!/bin/bash

# Record start time
start_time=$(date +%s)

# Path to bbmerge.sh
BBMERGE="./bbmerge.sh"

# Directory containing the FASTQ files
FASTQ_DIR="./trimmed_sequences"

# Output directory
OUTPUT_DIR="./merged_sequences"

# Iterate over each forward read file
for R1_FILE in "$FASTQ_DIR"/*_R1_val_1.fq.gz; do
    # Extract the base filename without extension
    BASENAME=$(basename "$R1_FILE" _R1_val_1.fq.gz)
    
    # Corresponding reverse read file
    R2_FILE="$FASTQ_DIR/${BASENAME}_R2_val_2.fq.gz"
    
    # Output file
    OUTPUT_FILE="$OUTPUT_DIR/${BASENAME}.fq.gz"
    
    # Run bbmerge.sh
    "$BBMERGE" in1="$R1_FILE" in2="$R2_FILE" out="$OUTPUT_FILE"
done

# Go to the directory containing the merged samples 
cd $OUTPUT_DIR

# Run FastQC on all trimmed FASTQ files
fastqc *.fq.gz

# Record end time
end_time=$(date +%s)

# Calculate the duration
duration=$((end_time - start_time))

#######################################################
##### 	        Select >64bp with cutadapt        #####
#######################################################

# Adapter trimming using cutadapt
#!/bin/bash

# Directory containing the FASTQ files
FASTQ_DIR="./merged_sequences"

# Output directory
OUTPUT_DIR="./cutadapt64bp"

# Loop through each FASTQ file in the FASTQ directory
for file in "$FASTQ_DIR"/*.fq.gz; do
    # Get the filename without extension
    filename=$(basename "$file" .fq.gz)
    # Trim adapters and discard reads shorter than 64 bases
    cutadapt -m 64 -o "$OUTPUT_DIR/${filename}_trimmed.fq.gz" "$file"
done

# Run FastQC on the trimmed samples
fastqc "$OUTPUT_DIR"/*.fq.gz

#######################################################
#####      Prepare your files for TASSEL          #####
#######################################################

# Add fake barcodes to FASTQ files for TASSEL5 GBSv2 compatibility

# This function adds fake barcodes to the start of reads in Illumina FASTQ files for compatibility with TASSEL5 GBSv2.
# fastq_dir is the directory path (string) of *unzipped* FASTQ files
# Returns unzipped FASTQ files with fake barcodes and quality scores, named according to the GBSv2 requirement,
# and a key, "barcode_key.csv", which matches original file name, new file name, flowcell, lane, and barcode
# All files are output to the working directory.
#
# Example usage:
barcode_faker(fastq_dir =  "./cutadapt64bp")

fastq_dir = "./cutadapt64bp"

# WARNINGS:
# Always run barcode_faker on a folder containing all files you intend to use in a given GBSv2 run. Multiple separate runs
# of barcode_faker can cause barcodes to be repeated across sample files (though unlikely), and will certainly cause
# file names (flowcell and lane) to be repeated.
# The barcodes generated by this function may recreate enzyme cut sites. The function is not intended to make barcodes for practical use.
# Never delete your original FASTQ files after running barcode_faker, and remember to back up the originals. 
# Marlee R. Labroo, dulynoted713@gmail.com.

fastqs <- list.files(path = fastq_dir, full.names = TRUE)
fastq.check <- list.files(path = fastq_dir, full.names = FALSE)
  
  #check that all files in dir are fastqs         #bug fixed 28 July 2020- reported by Dr. Smit Dhakal
  long <- length(grep(".fastq", fastq.check))
  short <- length(grep(".fq", fastq.check))
  if(long != 0){
    if(length(fastq.check) != long){
      stop("Not all of the files in your input folder seem to have .fq or .fastq extensions. Please move or delete such files.")
    }
  }
  if(short != 0){
    if(length(fastq.check) != short){
      stop("Not all of the files in your input folder seem to have .fq or .fastq extensions. Please move or delete such files.")
    }
  }
  
  # new method: given a number of input files, determine the min barcode length needed to ensure enough unique barcodes are generated
  # the old way failed if a lot of files had short barcodes of the same length- reported 18 Aug 2022 by Meghan Brady
  # https://www.calculatorsoup.com/calculators/discretemathematics/combinationsreplacement.php
  # n = 4
  # r = min barcode length
  # C = number of files
  # solve for r considering n and C are given
  C <- length(fastqs)
  constant <- (factorial(3) * C) - 6 # get the constant
  r <- round(polyroot(c(constant, -11, -6, -1)), 2) # get the roots of the polynomial
  r <- ceiling(Re(r)[Re(r) > 0]) # subset the real and positive roots, and round up
  if(r < 4){r <- 4} # keep a minimum barcode length of 4, probably unnecessary


  #this function recursively makes a vector of unique fake barcodes to ensure no non-unique barcodes are generated
  #this step does not protect against enzyme cut site being found in the barcode
  barcode_inventor <- function(fake_barcodes, r){
    fake_barcodes <- c()
    
    # make some barcodes of the length needed to ensure unique barcodes for all files are possible
    for(i in 1:length(fastqs)){
      fake_barcodes[i] <- paste(sample(c("A", "T", "C", "G"), size = r, replace = TRUE), collapse = "")
    }
    
    # check if the barcodes are all unique
    if(length(fake_barcodes) == length(unique(fake_barcodes))){
      return(fake_barcodes)
    } else {
      barcode_inventor
    }
  }
  fake_barcodes <- barcode_inventor(fake_barcodes, r)
  
  #make the fake perfect quality scores matching the barcode length
  fake_quals <- rep(paste(rep("E", times = r), collapse = ""), times = length(fastqs))         
  
  #make fake header files for FASTQ reads
  #headers are totally fake (do not refer to input file)
  #fake headers allows interoperability between TASSEL and demultiplexing software FASTQ formats
  fake_flowcell_no <- seq(from = 1, to = length(fastqs), by = 1)
  fake_headers <- paste("@D00553R:56:", "C8B56ANXW", fake_flowcell_no, ":3:1101:1203:2037 1:N:0:3", sep = "") # I have added a @ to be able to run fastqc after this - Celia Vinagre Izquierdo 
  fake_flowcells <- paste("C8B56ANXW", fake_flowcell_no, sep = "")
  fake_lanes <- rep(3, times = length(fastqs))
  
  #make fake file names that appropriately reference fake flowcell and lane
  newfile_names <- paste(fake_flowcells, "_", "3","_","fastq.fastq", sep = "")
  
  #stop if files with the fake file names already exist
  workdir <- getwd()
  dirfil <- list.files(path = workdir, full.names = FALSE)
  if(sum(newfile_names %in% dirfil) != 0){
    stop("Files in your working directory seem to have same names as those
         output by this function. Please move them, delete them, or choose a new working directory.")
  }
  
  #write a key of barcode, flowcell, lane and new file names
  key <- cbind(fastqs, newfile_names, fake_flowcells, fake_lanes, fake_barcodes)
  write.csv(key, "fakebarcodes_key_gbs1.csv", row.names = FALSE)
  
  #read 400 lines per file at a time, paste on the barcodes and quality scores, make unique flowcells, 
  #and write them out with unique names
  for(i in 1:length(fastqs)){
    incon <- file(fastqs[i], open = "r")
    outcon <- file(newfile_names[i], open = "w")
    while(length(mylines <- readLines(incon, n = 400, warn = FALSE))){
      head_pos <- seq(1, length(mylines), by = 4)
      seq_pos <- seq(2, length(mylines), by = 4)
      qual_pos <- seq(4, length(mylines), by = 4)
      
      for(j in 1:length(mylines)){
        mylines[head_pos[j]] <- fake_headers[i]
        mylines[seq_pos[j]] <- paste(fake_barcodes[i], mylines[seq_pos[j]], sep = "")
        mylines[qual_pos[j]] <- paste(fake_quals[i], mylines[qual_pos[j]], sep = "")
      }
      writeLines(mylines, outcon)
    }
    print(paste("Finished writing File", i, "...", sep = " "))
    #close the connections
    close(outcon, warn = FALSE)
    close(incon, warn = FALSE)
  }
}


#######################################################
##### 	              Use TASSEL                  #####
#######################################################

# Reference genome preparation
wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/001/577/835/GCF_001577835.2_Coturnix_japonica_2.1/GCF_001577835.2_Coturnix_japonica_2.1_genomic.fna.gz
gunzip GCF_001577835.2_Coturnix_japonica_2.1_genomic.fna.gz

conda activate /opt/miniconda3/envs/bowtie2

bowtie2-build ./refGenomejaponica/GCF_001577835.2_Coturnix_japonica_2.1_genomic.fna PN40024v2

mkdir sequences
mkdir alignment
mkdir keyfile

# GBS analysis steps using TASSEL

# Run GBSSeqToTagDBPlugin

time /opt/tassel5-TEGenzymes/run_pipeline.pl -Xms1G -Xmx12G -fork1 -GBSSeqToTagDBPlugin -e TEGEcoT22I  -i sequences/ -db ./wintering.db -k ./keyfile/snp_wintering.txt  -kmerLength 64 -minKmerL 20 -mnQS 20 -mxKmerNum 100000000 -batchSize 31 -endPlugin -runfork1 2>&1 | tee -a ./wintering.log

# Run TagExportToFastqPlugin

time /opt/tassel5-TEGenzymes/run_pipeline.pl -Xms1G -Xmx12G -fork1 -TagExportToFastqPlugin -db ./wintering.db -o ./tagsForAlign.fa.gz -c 5 -endPlugin -runfork1 2>&1 | tee -a ./wintering.log

# Align

conda activate /opt/miniconda3/envs/bowtie2

time bowtie2 -p 14 --very-sensitive -x PN40024v2 -U tagsForAlign.fa.gz  -S ./alignment/tagsForAlignFullvs.sam 2>&1 | tee -a ./wintering.log

# run SAMToGBSdbPlugin Creates the initial DB of tags

time /opt/tassel5-TEGenzymes/run_pipeline.pl -Xms1G -Xmx12G -fork1 -SAMToGBSdbPlugin -i ./alignment/tagsForAlignFullvs.sam -db ./wintering.db -aProp 0.0 -aLen 0 -endPlugin -runfork1 2>&1 | tee -a ./wintering.log
# run DiscoverySNPCallerPluginV2

time /opt/tassel5-TEGenzymes/run_pipeline.pl -Xms512m -Xmx35G -fork1 -DiscoverySNPCallerPluginV2 -db ./wintering.db -sC "NC_029516.1" -eC "NC_029547.1" -mnLCov 0.1 -deleteOldData true  -endPlugin  -runfork1

# run ProductionSNPCallerPluginV2

time /opt/tassel5-TEGenzymes/run_pipeline.pl -Xms1G -Xmx12G -fork1 -ProductionSNPCallerPluginV2 -db ./wintering.db -e TEGEcoT22I -i ./sequences -k ./keyfile/snp_wintering.txt -kmerLength 64 -o ./wintering-snp.vcf -batchSize 31 -endPlugin -runfork1 2>&1 | tee -a ./wintering.log

# run GetTagTaxaDistFromDBPlugin

time /opt/tassel5-TEGenzymes/run_pipeline.pl -Xms1G -Xmx12G -fork1 -GetTagTaxaDistFromDBPlugin -db ./wintering.db -o ./gbsTagTaxaDistOutput.txt -endPlugin -runfork1 2>&1 | tee -a ./wintering.log

# run the genotypeSummary plugin to generate summary statistics.

time /opt/tassel5-TEGenzymes/run_pipeline.pl -Xmx12g -importGuess wintering-snp.vcf -genotypeSummary overall,site -export genotypeSummary 

# Add some things to GZIP the output files from the above steps.

gzip *.vcf

gzip ./alignment/*

gzip *.txt


#######################################################
##### 	    Filter VCF and generate PCA           #####
#######################################################

# Filter VCF file using VCFtools

vcftools --vcf wintering-snp.vcf --maf 0.03 --max-alleles 2 --maxDP 100 --min-alleles 2 --minDP 5 --max-missing 0.75 --remove-indels --recode --recode-INFO-all --out wintering_maf003_miss075

# Do a PCA with all the genome

/usr/local/bin/plink --vcf wintering_maf003_miss075.recode.vcf --pca --allow-extra-chr --chr-set 31 --out wintering_maf003_miss075

# Filter to see your samples and controls

vcftools --vcf wintering_maf003_miss075.recode.vcf --keep samples_control.txt --recode --out samples_control

# Subset the inversion in chromosome 1

vcftools --vcf samples_control.recode.vcf --chr NC_029516.1 --from-bp 52100000 --to-bp 168610000 --out inv1_samples_control --recode

# Do a PCA with the inversion subset

/usr/local/bin/plink --vcf inv1_samples_control.recode.vcf --pca --allow-extra-chr --chr-set 1 --out inv1_samples_control

# Continue in R to plot the PCA